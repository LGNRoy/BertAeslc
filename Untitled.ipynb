{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\study\\Capstone\\bertAeslc\\src\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from others.logging import init_logger\n",
    "from train_abstractive import validate_abs, train_abs, baseline, test_abs, test_text_abs\n",
    "from train_extractive import train_ext, validate_ext, test_ext\n",
    "\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'emb_size', 'enc_layers', 'enc_hidden_size', 'enc_ff_size',\n",
    "               'dec_layers', 'dec_hidden_size', 'dec_ff_size', 'encoder', 'ff_actv', 'use_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-task\", default='ext', type=str, choices=['ext', 'abs'])\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
    "parser.add_argument(\"-mode\", default='train', type=str, choices=['train', 'validate', 'test'])\n",
    "parser.add_argument(\"-bert_data_path\", default='../bert_data_new/cnndm')\n",
    "parser.add_argument(\"-model_path\", default='../models/')\n",
    "parser.add_argument(\"-result_path\", default='../results/cnndm')\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "\n",
    "parser.add_argument(\"-batch_size\", default=140, type=int)\n",
    "parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
    "\n",
    "parser.add_argument(\"-max_pos\", default=512, type=int)\n",
    "parser.add_argument(\"-use_interval\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-large\", type=bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
    "\n",
    "parser.add_argument(\"-sep_optim\", type=bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-use_bert_emb\", type=bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument(\"-share_emb\", type=bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "\n",
    "# params for EXT\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "parser.add_argument(\"-beam_size\", default=5, type=int)\n",
    "parser.add_argument(\"-min_length\", default=15, type=int)\n",
    "parser.add_argument(\"-max_length\", default=150, type=int)\n",
    "parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-save_checkpoint_steps\", default=5, type=int)\n",
    "parser.add_argument(\"-accum_count\", default=1, type=int)\n",
    "parser.add_argument(\"-report_every\", default=1, type=int)\n",
    "parser.add_argument(\"-train_steps\", default=1000, type=int)\n",
    "parser.add_argument(\"-recall_eval\", type=bool, nargs='?',const=True,default=False)\n",
    "\n",
    "\n",
    "parser.add_argument('-visible_gpus', default='-1', type=str)\n",
    "parser.add_argument('-gpu_ranks', default='0', type=str)\n",
    "parser.add_argument('-log_file', default='../logs/cnndm.log')\n",
    "parser.add_argument('-seed', default=666, type=int)\n",
    "\n",
    "parser.add_argument(\"-test_all\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-test_from\", default='')\n",
    "parser.add_argument(\"-test_start_from\", default=-1, type=int)\n",
    "\n",
    "parser.add_argument(\"-train_from\", default='')\n",
    "parser.add_argument(\"-report_rouge\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-block_trigram\", type=bool, nargs='?', const=True, default=True)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task = abs \n",
    "args.mode = \"train\" \n",
    "args.bert_data_path = \"../bert_data/bert\"\n",
    "args.dec_dropout = 0.2  \n",
    "args.model_path = \"../models/absext\"\n",
    "args.sep_optim = True \n",
    "args.lr_bert = 0.002 \n",
    "args.lr_dec = 0.2 \n",
    "args.save_checkpoint_steps = 2000 \n",
    "args.batch_size = 140 \n",
    "args.train_steps = 200000 \n",
    "args.report_every = 50 \n",
    "args.accum_count = 5 \n",
    "args.use_bert_emb = True \n",
    "args.use_interval = True \n",
    "args.warmup_steps_bert = 20000 \n",
    "args.warmup_steps_dec = 10000 \n",
    "args.max_pos = 512 \n",
    "args.visible_gpus = \"-1\" \n",
    "args.log_file = \"../logs/abs_bert_cnndm\"  \n",
    "args.load_from_extractive = \"../models/ext/ext_model_18000.pt\"\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in range(len(args.visible_gpus.split(',')))]\n",
    "args.world_size = len(args.gpu_ranks)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-04 17:23:56,361 INFO] Namespace(accum_count=5, alpha=0.95, batch_size=140, beam_size=5, bert_data_path='../bert_data/bert', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='../models/ext/ext_model_18000.pt', log_file='../logs/abs_bert_cnndm', lr=1, lr_bert=0.002, lr_dec=0.2, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='train', model_path='../models/absext', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=50, report_rouge=True, result_path='../logs/abs_bert_aeslc', save_checkpoint_steps=2000, seed=666, sep_optim=True, share_emb=False, task=<built-in function abs>, temp_dir='../temp', test_all=True, test_batch_size=500, test_from='', test_start_from=-1, train_from='', train_steps=200000, use_bert_emb=True, use_interval=True, visible_gpus='-1', warmup_steps=8000, warmup_steps_bert=20000, warmup_steps_dec=10000, world_size=1)\n",
      "[2019-10-04 17:23:56,362 INFO] Device ID -1\n",
      "[2019-10-04 17:23:56,363 INFO] Device cpu\n",
      "[2019-10-04 17:23:56,366 INFO] Loading bert from extractive model ../models/ext/ext_model_18000.pt\n",
      "[2019-10-04 17:23:57,974 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "[2019-10-04 17:23:57,975 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2019-10-04 17:23:58,821 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2019-10-04 17:24:03,832 INFO] AbsSummarizer(\n",
      "  (bert): Bert(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (pos_emb): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (1): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-10-04 17:24:04,745 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-04 17:24:04,841 INFO] * number of parameters: 180222522\n",
      "[2019-10-04 17:24:04,843 INFO] Start training...\n",
      "[2019-10-04 17:24:05,681 INFO] Loading train dataset from ../bert_data/bert.train.pt, number of examples: 14436\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `bitwise_not()` operator instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4aba70468d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_abs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\train_abstractive.py\u001b[0m in \u001b[0;36mtrain_abs\u001b[1;34m(args, device_id)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mtrain_abs_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mtrain_abs_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\train_abstractive.py\u001b[0m in \u001b[0;36mtrain_abs_single\u001b[1;34m(args, device_id)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter_fct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\models\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_iter_fct, train_steps, valid_iter_fct, valid_steps)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mreduce_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\models\\data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mdataset_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\models\\data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\study\\Capstone\\bertAeslc\\src\\models\\data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, device, is_test)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0msegs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_segs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mmask_src\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mmask_tgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `bitwise_not()` operator instead."
     ]
    }
   ],
   "source": [
    "train_abs(args, device_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
