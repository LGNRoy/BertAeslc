{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from others.logging import init_logger\n",
    "\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'emb_size', 'enc_layers', 'enc_hidden_size', 'enc_ff_size',\n",
    "               'dec_layers', 'dec_hidden_size', 'dec_ff_size', 'encoder', 'ff_actv', 'use_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-task\", default='ext', type=str, choices=['ext', 'abs'])\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
    "parser.add_argument(\"-mode\", default='train', type=str, choices=['train', 'validate', 'test'])\n",
    "parser.add_argument(\"-bert_data_path\", default='../bert_data_new/cnndm')\n",
    "parser.add_argument(\"-model_path\", default='../models/')\n",
    "parser.add_argument(\"-result_path\", default='../results/cnndm')\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "\n",
    "parser.add_argument(\"-batch_size\", default=140, type=int)\n",
    "parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
    "\n",
    "parser.add_argument(\"-max_pos\", default=512, type=int)\n",
    "parser.add_argument(\"-use_interval\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-large\", type=bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
    "\n",
    "parser.add_argument(\"-sep_optim\", type=bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-use_bert_emb\", type=bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument(\"-share_emb\", type=bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "\n",
    "# params for EXT\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "parser.add_argument(\"-beam_size\", default=5, type=int)\n",
    "parser.add_argument(\"-min_length\", default=15, type=int)\n",
    "parser.add_argument(\"-max_length\", default=150, type=int)\n",
    "parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-save_checkpoint_steps\", default=5, type=int)\n",
    "parser.add_argument(\"-accum_count\", default=1, type=int)\n",
    "parser.add_argument(\"-report_every\", default=1, type=int)\n",
    "parser.add_argument(\"-train_steps\", default=1000, type=int)\n",
    "parser.add_argument(\"-recall_eval\", type=bool, nargs='?',const=True,default=False)\n",
    "\n",
    "\n",
    "parser.add_argument('-visible_gpus', default='-1', type=str)\n",
    "parser.add_argument('-gpu_ranks', default='0', type=str)\n",
    "parser.add_argument('-log_file', default='../logs/cnndm.log')\n",
    "parser.add_argument('-seed', default=666, type=int)\n",
    "\n",
    "parser.add_argument(\"-test_all\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-test_from\", default='')\n",
    "parser.add_argument(\"-test_start_from\", default=-1, type=int)\n",
    "\n",
    "parser.add_argument(\"-train_from\", default='')\n",
    "parser.add_argument(\"-report_rouge\", type=bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-block_trigram\", type=bool, nargs='?', const=True, default=True)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task = abs \n",
    "args.mode = \"train\" \n",
    "args.bert_data_path = \"../../../dataset/aeslc_name/random_bert/AESLC.bert\"\n",
    "args.dec_dropout = 0.2  \n",
    "args.model_path = \"\"\n",
    "args.sep_optim = True \n",
    "args.lr_bert = 0.002 \n",
    "args.lr_dec = 0.2 \n",
    "args.save_checkpoint_steps = 2000 \n",
    "args.batch_size = 140 \n",
    "args.train_steps = 200000 \n",
    "args.report_every = 50 \n",
    "args.accum_count = 5 \n",
    "args.use_bert_emb = True \n",
    "args.use_interval = True \n",
    "args.warmup_steps_bert = 20000 \n",
    "args.warmup_steps_dec = 10000 \n",
    "args.max_pos = 512 \n",
    "args.visible_gpus = \"-1\" \n",
    "args.log_file = \"\"  \n",
    "args.load_from_extractive = \"\"\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in range(len(args.visible_gpus.split(',')))]\n",
    "args.world_size = len(args.gpu_ranks)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-22 13:28:21,754 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, cache_dir=args.temp_dir)\n",
    "symbols = {'BOS': tokenizer.vocab['[unused0]'], 'EOS': tokenizer.vocab['[unused1]'],\n",
    "           'PAD': tokenizer.vocab['[PAD]'], 'EOQ': tokenizer.vocab['[unused2]']}\n",
    "def decode(line):\n",
    "    print(\" \".join([tokenizer.ids_to_tokens[int(n)] for n in line]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import gc\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from others.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_batch_size_fn(new, count):\n",
    "    src, tgt = new[0], new[1]\n",
    "    global max_n_sents, max_n_tokens, max_size\n",
    "    if count == 1:\n",
    "        max_size = 0\n",
    "        max_n_sents=0\n",
    "        max_n_tokens=0\n",
    "    max_n_sents = max(max_n_sents, len(tgt))\n",
    "    max_size = max(max_size, max_n_sents)\n",
    "    src_elements = count * max_size\n",
    "    if (count > 6):\n",
    "        return src_elements + 1e3\n",
    "    return src_elements\n",
    "\n",
    "\n",
    "def ext_batch_size_fn(new, count):\n",
    "    if (len(new) == 4):\n",
    "        pass\n",
    "    src, labels = new[0], new[4]\n",
    "    global max_n_sents, max_n_tokens, max_size\n",
    "    if count == 1:\n",
    "        max_size = 0\n",
    "        max_n_sents = 0\n",
    "        max_n_tokens = 0\n",
    "    max_n_sents = max(max_n_sents, len(src))\n",
    "    max_size = max(max_size, max_n_sents)\n",
    "    src_elements = count * max_size\n",
    "    return src_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def _pad(self, data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "\n",
    "    def __init__(self, data=None, device=None, is_test=False):\n",
    "        \"\"\"Create a Batch from a list of examples.\"\"\"\n",
    "        if data is not None:\n",
    "            self.batch_size = len(data)\n",
    "            pre_src = [x[0] for x in data]\n",
    "            pre_tgt = [x[1] for x in data]\n",
    "            pre_segs = [x[2] for x in data]\n",
    "            pre_clss = [x[3] for x in data]\n",
    "            pre_src_sent_labels = [x[4] for x in data]\n",
    "\n",
    "            src = torch.tensor(self._pad(pre_src, 0))\n",
    "            tgt = torch.tensor(self._pad(pre_tgt, 0))\n",
    "\n",
    "            segs = torch.tensor(self._pad(pre_segs, 0))\n",
    "            mask_src = 1 - (src == 0)\n",
    "            mask_tgt = 1 - (tgt == 0)\n",
    "\n",
    "\n",
    "            clss = torch.tensor(self._pad(pre_clss, -1))\n",
    "            src_sent_labels = torch.tensor(self._pad(pre_src_sent_labels, 0))\n",
    "            mask_cls = 1 - (clss == -1)\n",
    "            clss[clss == -1] = 0\n",
    "            setattr(self, 'clss', clss.to(device))\n",
    "            setattr(self, 'mask_cls', mask_cls.to(device))\n",
    "            setattr(self, 'src_sent_labels', src_sent_labels.to(device))\n",
    "\n",
    "\n",
    "            setattr(self, 'src', src.to(device))\n",
    "            setattr(self, 'tgt', tgt.to(device))\n",
    "            setattr(self, 'segs', segs.to(device))\n",
    "            setattr(self, 'mask_src', mask_src.to(device))\n",
    "            setattr(self, 'mask_tgt', mask_tgt.to(device))\n",
    "\n",
    "\n",
    "            if (is_test):\n",
    "                src_str = [x[-2] for x in data]\n",
    "                setattr(self, 'src_str', src_str)\n",
    "                tgt_str = [x[-1] for x in data]\n",
    "                setattr(self, 'tgt_str', tgt_str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, args, datasets,  batch_size,\n",
    "                 device, shuffle, is_test):\n",
    "        print(1.1)\n",
    "        self.args = args\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.is_test = is_test\n",
    "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
    "        assert self.cur_iter is not None\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset_iter = (d for d in self.datasets)\n",
    "        while self.cur_iter is not None:\n",
    "            for batch in self.cur_iter:\n",
    "                yield batch\n",
    "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
    "\n",
    "\n",
    "    def _next_dataset_iterator(self, dataset_iter):\n",
    "        try:\n",
    "            # Drop the current dataset for decreasing memory\n",
    "            if hasattr(self, \"cur_dataset\"):\n",
    "                self.cur_dataset = None\n",
    "                gc.collect()\n",
    "                del self.cur_dataset\n",
    "                gc.collect()\n",
    "                \n",
    "            self.cur_dataset = next(dataset_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        return DataIterator(args = self.args,\n",
    "            dataset=self.cur_dataset,  batch_size=self.batch_size,\n",
    "            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n",
    "\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, args, dataset,  batch_size, device=None, is_test=False,\n",
    "                 shuffle=True):\n",
    "        self.args = args\n",
    "        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n",
    "        self.iterations = 0\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.sort_key = lambda x: len(x[1])\n",
    "\n",
    "        self._iterations_this_epoch = 0\n",
    "        if (self.args.task == 'abs'):\n",
    "            self.batch_size_fn = abs_batch_size_fn\n",
    "        else:\n",
    "            self.batch_size_fn = ext_batch_size_fn\n",
    "\n",
    "    def data(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset)\n",
    "        xs = self.dataset\n",
    "        return xs\n",
    "\n",
    "    def preprocess(self, ex, is_test):\n",
    "        src = ex['src']\n",
    "        tgt = ex['tgt'][:self.args.max_tgt_len][:-1]+[2]\n",
    "        src_sent_labels = ex['src_sent_labels']\n",
    "        segs = ex['segs']\n",
    "        if(not self.args.use_interval):\n",
    "            segs=[0]*len(segs)\n",
    "        clss = ex['clss']\n",
    "        src_txt = ex['src_txt']\n",
    "        tgt_txt = ex['tgt_txt']\n",
    "\n",
    "        end_id = [src[-1]]\n",
    "        src = src[:-1][:self.args.max_pos - 1] + end_id\n",
    "        segs = segs[:self.args.max_pos]\n",
    "        max_sent_id = bisect.bisect_left(clss, self.args.max_pos)\n",
    "        src_sent_labels = src_sent_labels[:max_sent_id]\n",
    "        clss = clss[:max_sent_id]\n",
    "        # src_txt = src_txt[:max_sent_id]\n",
    "\n",
    "        if(is_test):\n",
    "            return src, tgt, segs, clss, src_sent_labels, src_txt, tgt_txt\n",
    "        else:\n",
    "            return src, tgt, segs, clss, src_sent_labels\n",
    "\n",
    "    def batch_buffer(self, data, batch_size):\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            if(len(ex['src'])==0):\n",
    "                continue\n",
    "            ex = self.preprocess(ex, self.is_test)\n",
    "            if(ex is None):\n",
    "                continue\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = len(minibatch)\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], 1\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "    def batch(self, data, batch_size):\n",
    "        \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = len(minibatch)\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], 1\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\" Create batches \"\"\"\n",
    "        data = self.data()\n",
    "        for buffer in self.batch_buffer(data, self.batch_size*2):\n",
    "            print(\"buffer\",len(buffer))\n",
    "            if (self.args.task == 'abs'):\n",
    "                p_batch = sorted(buffer, key=lambda x: len(x[2]))\n",
    "                p_batch = sorted(p_batch, key=lambda x: len(x[1]))\n",
    "            else:\n",
    "                p_batch = sorted(buffer, key=lambda x: len(x[2]))\n",
    "\n",
    "            p_batch = self.batch(p_batch, self.batch_size)\n",
    "\n",
    "            p_batch = list(p_batch)\n",
    "            print(\"p_batch\", len(p_batch))\n",
    "            if (self.shuffle):\n",
    "                random.shuffle(p_batch)\n",
    "            for b in p_batch:\n",
    "                if(len(b)==0):\n",
    "                    continue\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.batches = self.create_batches()\n",
    "            for idx, minibatch in enumerate(self.batches):\n",
    "                # fast-forward if loaded from state\n",
    "                if self._iterations_this_epoch > idx:\n",
    "                    continue\n",
    "                self.iterations += 1\n",
    "                self._iterations_this_epoch += 1\n",
    "                batch = Batch(minibatch, self.device, self.is_test)\n",
    "\n",
    "                yield batch\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter_fct():\n",
    "    return data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n",
    "                                  shuffle=True, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-22 13:29:41,824 INFO] Loading train dataset from ../../../dataset/aeslc_name/random_bert/AESLC.bert.train.pt, number of examples: 14499\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iter_fct()):\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  14,   0,  ...,   0,   0,   0],\n",
       "        [  0,  23,   0,  ...,   0,   0,   0],\n",
       "        [  0,   4,  18,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,  61,  88,  ...,   0,   0,   0],\n",
       "        [  0,   4,  14,  ...,   0,   0,   0],\n",
       "        [  0,   6,  20,  ..., 110,   0,   0]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation form for the scott wa ##ddle interview held on tuesday , september 4 , 2001 [unused1]\n"
     ]
    }
   ],
   "source": [
    "decode(batch.tgt[:, 1:][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
